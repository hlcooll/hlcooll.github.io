<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kubernetes on hlcooll</title>
    <link>https://hlcooll.github.io/public/kubernetes/</link>
    <description>Recent content in Kubernetes on hlcooll</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 16 Oct 2020 10:12:53 +0800</lastBuildDate>
    
	<atom:link href="https://hlcooll.github.io/public/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Graylogs</title>
      <link>https://hlcooll.github.io/public/kubernetes/helm/graylogs/</link>
      <pubDate>Fri, 16 Oct 2020 10:12:53 +0800</pubDate>
      
      <guid>https://hlcooll.github.io/public/kubernetes/helm/graylogs/</guid>
      <description>#mongodb
# Warning FailedCreate 30s (x14 over 71s) statefulset-controller create Pod mongodb-replicaset-1602575807-0 in StatefulSet mongodb-replicaset-1602575807 failed error: Pod &amp;quot;mongodb-replicaset-1602575807-0&amp;quot; is invalid: spec.containers[0].startupProbe.successThreshold: Invalid value: 2: must be 1 helm fetch stable/mongodb-replicaset kubectl create namespace mongodb helm install mongodb-replicaset --namespace &amp;quot;graylog&amp;quot; -n &amp;quot;mongodb&amp;quot; stable/mongodb-replicaset --set persistentVolume.storageClass=&amp;quot;nfs-client&amp;quot; --set startupProbe.successThreshold=&amp;quot;1&amp;quot; WARNING: This chart is deprecated NAME: mongodb-replicaset LAST DEPLOYED: Fri Oct 16 10:17:48 2020 NAMESPACE: mongodb STATUS: deployed REVISION: 1 NOTES: ******************* ****DEPRECATED***** ******************* * * This chart is deprecated and no longer maintained.</description>
    </item>
    
    <item>
      <title>Helm</title>
      <link>https://hlcooll.github.io/public/kubernetes/helm/helm/</link>
      <pubDate>Thu, 15 Oct 2020 18:03:52 +0800</pubDate>
      
      <guid>https://hlcooll.github.io/public/kubernetes/helm/helm/</guid>
      <description>#安装Helm3
#下载： wget https://get.helm.sh/helm-v3.3.1-linux-amd64.tar.gz curl -L -o helm-v3.2.4-linux-amd64.tar.gz https://file.choerodon.com.cn/kubernetes-helm/v3.2.4/helm-v3.2.4-linux-amd64.tar.gz #官方安装文档 curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get &amp;gt; get_helm.sh 解压压缩包（以linux-amd64为例） tar -zxvf helm-v3.2.4-linux-amd64.tar.gz 将文件移动到PATH目录中（以linux-amd64为例） sudo mv linux-amd64/helm /usr/bin/helm chmod a+x /usr/bin/helm #验证部署 执行命令，出现以下信息即部署成功。 helm version #配置仓库 helm repo add aliyun https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts </description>
    </item>
    
    <item>
      <title>Nfs Sc</title>
      <link>https://hlcooll.github.io/public/kubernetes/helm/nfs-sc/</link>
      <pubDate>Thu, 15 Oct 2020 18:02:24 +0800</pubDate>
      
      <guid>https://hlcooll.github.io/public/kubernetes/helm/nfs-sc/</guid>
      <description>https://github.com/kubernetes-retired/external-storage/tree/master/nfs-client
#创建nfs-sc helm install stable/nfs-client-provisioner &amp;ndash;set nfs.server=172.17.16.14 &amp;ndash;set nfs.path=/data/u01/prod</description>
    </item>
    
    <item>
      <title>Ingress Tls</title>
      <link>https://hlcooll.github.io/public/kubernetes/ingress-tls/</link>
      <pubDate>Thu, 15 Oct 2020 17:31:29 +0800</pubDate>
      
      <guid>https://hlcooll.github.io/public/kubernetes/ingress-tls/</guid>
      <description>kubectl -n kubernetes-dashboard create secret tls hl-https --cert=hlcooll.crt --key=hlcooll.key apiVersion: extensions/v1beta1 kind: Ingress metadata: name: kubernetes-dashboard spec: tls: - hosts: - dashboard.hlcool.top secretName: https rules: - host: sslexample.foo.com http: paths: - path: / backend: serviceName: kubernetes-dashboard servicePort: 80 </description>
    </item>
    
    <item>
      <title>K8s Affinity</title>
      <link>https://hlcooll.github.io/public/kubernetes/k8s-affinity/</link>
      <pubDate>Mon, 12 Oct 2020 11:15:57 +0800</pubDate>
      
      <guid>https://hlcooll.github.io/public/kubernetes/k8s-affinity/</guid>
      <description>k8s学习笔记-调度之Affinity Kubernetes中的调度策略可以大致分为两种 一种是全局的调度策略，要在启动调度器时配置，包括kubernetes调度器自带的各种predicates和priorities算法，具体可以参看上一篇文章； 另一种是运行时调度策略，包括nodeAffinity（主机亲和性），podAffinity（POD亲和性）以及podAntiAffinity（POD反亲和性）。 nodeAffinity 主要解决POD要部署在哪些主机，以及POD不能部署在哪些主机上的问题，处理的是POD和主机之间的关系。 podAffinity 主要解决POD可以和哪些POD部署在同一个拓扑域中的问题（拓扑域用主机标签实现，可以是单个主机，也可以是多个主机组成的cluster、zone等。） podAntiAffinity主要解决POD不能和哪些POD部署在同一个拓扑域中的问题。它们处理的是Kubernetes集群内部POD和POD之间的关系。 三种亲和性和反亲和性策略的比较如下表所示： 策略名称	匹配目标	支持的操作符	支持拓扑域	设计目标 nodeAffinity	主机标签	In，NotIn，Exists，DoesNotExist，Gt，Lt	不支持	决定Pod可以部署在哪些主机上 podAffinity	Pod标签	In，NotIn，Exists，DoesNotExist	支持	决定Pod可以和哪些Pod部署在同一拓扑域 PodAntiAffinity	Pod标签	In，NotIn，Exists，DoesNotExist	支持	决定Pod不可以和哪些Pod部署在同一拓扑域 亲和性：应用A与应用B两个应用频繁交互，所以有必要利用亲和性让两个应用的尽可能的靠近，甚至在一个node上，以减少因网络通信而带来的性能损耗。 反亲和性：当应用的采用多副本部署时，有必要采用反亲和性让各个应用实例打散分布在各个node上，以提高HA。 主要介绍kubernetes的中调度算法中的Node affinity和Pod affinity用法 实际上是对前文提到的优选策略中的NodeAffinityPriority策略和InterPodAffinityPriority策略的具体应用。 kubectl explain pods.spec.affinity 亲和性策略（Affinity）能够提供比NodeSelector或者Taints更灵活丰富的调度方式，例如： 丰富的匹配表达式（In, NotIn, Exists, DoesNotExist. Gt, and Lt） 软约束和硬约束（Required/Preferred） 以节点上的其他Pod作为参照物进行调度计算 亲和性策略分为NodeAffinityPriority策略和InterPodAffinityPriority策略。 先回顾一下之前的节点选择器 节点选择器： nodeSelector nodeName 创建一个Pod 节点选择器标签 nodeSelector: disktype: ssd 默认节点没这个标签：所以会调度失败 [root@k8s-master schedule]# kubectl get node --show-labels|egrep disktype [root@k8s-master schedule]# kubectl get pods NAME READY STATUS RESTARTS AGE pod-demo 0/1 Pending 0 11s [root@k8s-master schedule]# kubectl describe pod pod-demo Warning FailedScheduling 28s (x2 over 28s) default-scheduler 0/3 nodes are available: 3 node(s) didn&#39;t match node selector.</description>
    </item>
    
    <item>
      <title>Kubernetes</title>
      <link>https://hlcooll.github.io/public/kubernetes/kubernetes/</link>
      <pubDate>Mon, 14 Sep 2020 17:43:50 +0800</pubDate>
      
      <guid>https://hlcooll.github.io/public/kubernetes/kubernetes/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>